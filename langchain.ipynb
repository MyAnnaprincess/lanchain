{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNkLYp1dODADJ1IqnI9xlMK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7D3V34yFUvHe","executionInfo":{"status":"ok","timestamp":1719477905099,"user_tz":-480,"elapsed":26715,"user":{"displayName":"mr li","userId":"02746934671961749824"}},"outputId":"ecc67f35-1825-4348-e122-c2dc8293c1b3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","path=\"/content/drive/My Drive/langchain\"\n","os.chdir(path)\n","os.listdir(path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1a0xYAEoUyhr","executionInfo":{"status":"ok","timestamp":1719477921655,"user_tz":-480,"elapsed":929,"user":{"displayName":"mr li","userId":"02746934671961749824"}},"outputId":"98b17f0b-88b3-4a51-9b3c-d54bb28eb6e3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['chatGPT调研报告.pdf', '.env', 'chroma_db', 'langchain.ipynb']"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["!pip install langchain-community langchain-zhipu python-dotenv pypdf pandas chromadb datasets ragas matplotlib"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n7ZyxoNrVjcW","executionInfo":{"status":"ok","timestamp":1719478002820,"user_tz":-480,"elapsed":58499,"user":{"displayName":"mr li","userId":"02746934671961749824"}},"outputId":"3432ead4-9b7e-4f42-eea4-413027d4b72f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain-community\n","  Downloading langchain_community-0.2.6-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langchain-zhipu\n","  Downloading langchain_zhipu-4.1.5-py3-none-any.whl (18 kB)\n","Collecting python-dotenv\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Collecting pypdf\n","  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n","Collecting chromadb\n","  Downloading chromadb-0.5.3-py3-none-any.whl (559 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m559.5/559.5 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets\n","  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ragas\n","  Downloading ragas-0.1.9-py3-none-any.whl (86 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.1/86.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.31)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.9.5)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Collecting langchain<0.3.0,>=0.2.6 (from langchain-community)\n","  Downloading langchain-0.2.6-py3-none-any.whl (975 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m975.5/975.5 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langchain-core<0.3.0,>=0.2.10 (from langchain-community)\n","  Downloading langchain_core-0.2.10-py3-none-any.whl (332 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.8/332.8 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langsmith<0.2.0,>=0.1.0 (from langchain-community)\n","  Downloading langsmith-0.1.82-py3-none-any.whl (127 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.4/127.4 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.25.2)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.31.0)\n","Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.4.1)\n","Collecting cachetools==4.2.2 (from langchain-zhipu)\n","  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n","Collecting httpx<0.24.0,>=0.23.0 (from langchain-zhipu)\n","  Downloading httpx-0.23.3-py3-none-any.whl (71 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-zhipu) (2.7.4)\n","Collecting pyjwt<3.0.0,>=2.8.0 (from langchain-zhipu)\n","  Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n","Requirement already satisfied: regex<2025.0.0,>=2024.4.16 in /usr/local/lib/python3.10/dist-packages (from langchain-zhipu) (2024.5.15)\n","Requirement already satisfied: tqdm<5.0.0,>=4.66.2 in /usr/local/lib/python3.10/dist-packages (from langchain-zhipu) (4.66.4)\n","Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.1)\n","Collecting chroma-hnswlib==0.7.3 (from chromadb)\n","  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n","  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n","  Downloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting posthog>=2.4.0 (from chromadb)\n","  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n","  Downloading onnxruntime-1.18.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_api-1.25.0-py3-none-any.whl (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.25.0-py3-none-any.whl (18 kB)\n","Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n","  Downloading opentelemetry_instrumentation_fastapi-0.46b0-py3-none-any.whl (11 kB)\n","Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_sdk-1.25.0-py3-none-any.whl (107 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.19.1)\n","Collecting pypika>=0.48.9 (from chromadb)\n","  Downloading PyPika-0.48.9.tar.gz (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting overrides>=7.3.1 (from chromadb)\n","  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.0)\n","Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.64.1)\n","Collecting bcrypt>=4.0.1 (from chromadb)\n","  Downloading bcrypt-4.1.3-cp39-abi3-manylinux_2_28_x86_64.whl (283 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.12.3)\n","Collecting kubernetes>=28.1.0 (from chromadb)\n","  Downloading kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting mmh3>=4.0.1 (from chromadb)\n","  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting orjson>=3.9.12 (from chromadb)\n","  Downloading orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hINFO: pip is looking at multiple versions of chromadb to determine which version is compatible with other requirements. This could take a while.\n","Collecting chromadb\n","  Downloading chromadb-0.5.2-py3-none-any.whl (559 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m559.5/559.5 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading chromadb-0.5.1-py3-none-any.whl (559 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m559.5/559.5 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading chromadb-0.5.0-py3-none-any.whl (526 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.3)\n","Collecting pyarrow>=15.0.0 (from datasets)\n","  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting requests<3,>=2 (from langchain-community)\n","  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting xxhash (from datasets)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n","Collecting tiktoken (from ragas)\n","  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langchain-openai (from ragas)\n","  Downloading langchain_openai-0.1.10-py3-none-any.whl (40 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting openai>1 (from ragas)\n","  Downloading openai-1.35.5-py3-none-any.whl (327 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.5/327.5 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pysbd>=0.3.4 (from ragas)\n","  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ragas) (1.6.0)\n","Collecting appdirs (from ragas)\n","  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n","Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.1.0)\n","Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.95.2->chromadb)\n","  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi>=0.95.2->chromadb)\n","  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n","Requirement already satisfied: jinja2>=2.11.2 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (3.1.4)\n","Collecting python-multipart>=0.0.7 (from fastapi>=0.95.2->chromadb)\n","  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n","Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi>=0.95.2->chromadb)\n","  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi>=0.95.2->chromadb)\n","  Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<0.24.0,>=0.23.0->langchain-zhipu) (2024.6.2)\n","Collecting httpcore<0.17.0,>=0.15.0 (from httpx<0.24.0,>=0.23.0->langchain-zhipu)\n","  Downloading httpcore-0.16.3-py3-none-any.whl (69 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting rfc3986[idna2008]<2,>=1.3 (from httpx<0.24.0,>=0.23.0->langchain-zhipu)\n","  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.24.0,>=0.23.0->langchain-zhipu) (1.3.1)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n","Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n","Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n","Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n","Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n","Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n","Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain<0.3.0,>=0.2.6->langchain-community)\n","  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n","Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.10->langchain-community)\n","  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n","Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12.1)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>1->ragas) (1.7.0)\n","Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n","  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n","Collecting importlib-metadata<=7.1,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n","  Downloading importlib_metadata-7.1.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.1)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.25.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.25.0-py3-none-any.whl (17 kB)\n","Collecting opentelemetry-proto==1.25.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_proto-1.25.0-py3-none-any.whl (52 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_instrumentation_asgi-0.46b0-py3-none-any.whl (14 kB)\n","Collecting opentelemetry-instrumentation==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_instrumentation-0.46b0-py3-none-any.whl (29 kB)\n","Collecting opentelemetry-semantic-conventions==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl (130 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting opentelemetry-util-http==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_util_http-0.46b0-py3-none-any.whl (6.9 kB)\n","Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n","Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n","Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n","Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n","  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n","Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n","  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-zhipu) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-zhipu) (2.18.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (13.7.1)\n","Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>1->ragas) (1.2.1)\n","Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi>=0.95.2->chromadb)\n","  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=7.1,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.19.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.2->fastapi>=0.95.2->chromadb) (2.1.5)\n","Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain-community)\n","  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.16.1)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n","Building wheels for collected packages: pypika\n","  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53726 sha256=19ca93fd44a826ceacea3551fe9b7ccc205fcc41b4dc69dca823166b811f0a39\n","  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n","Successfully built pypika\n","Installing collected packages: rfc3986, pypika, monotonic, mmh3, appdirs, xxhash, websockets, uvloop, ujson, requests, python-multipart, python-dotenv, pysbd, pypdf, pyjwt, pyarrow, overrides, orjson, opentelemetry-util-http, opentelemetry-proto, mypy-extensions, marshmallow, jsonpointer, importlib-metadata, humanfriendly, httptools, h11, dnspython, dill, deprecated, chroma-hnswlib, cachetools, bcrypt, backoff, asgiref, watchfiles, uvicorn, typing-inspect, tiktoken, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, multiprocess, jsonpatch, httpcore, email_validator, coloredlogs, opentelemetry-semantic-conventions, opentelemetry-instrumentation, onnxruntime, langsmith, kubernetes, httpx, dataclasses-json, opentelemetry-sdk, opentelemetry-instrumentation-asgi, openai, langchain-core, fastapi-cli, datasets, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-grpc, langchain-text-splitters, langchain-openai, fastapi, langchain, chromadb, langchain-community, ragas, langchain-zhipu\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.31.0\n","    Uninstalling requests-2.31.0:\n","      Successfully uninstalled requests-2.31.0\n","  Attempting uninstall: pyjwt\n","    Found existing installation: PyJWT 2.3.0\n","    Uninstalling PyJWT-2.3.0:\n","      Successfully uninstalled PyJWT-2.3.0\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 14.0.2\n","    Uninstalling pyarrow-14.0.2:\n","      Successfully uninstalled pyarrow-14.0.2\n","  Attempting uninstall: importlib-metadata\n","    Found existing installation: importlib_metadata 7.2.0\n","    Uninstalling importlib_metadata-7.2.0:\n","      Successfully uninstalled importlib_metadata-7.2.0\n","  Attempting uninstall: cachetools\n","    Found existing installation: cachetools 5.3.3\n","    Uninstalling cachetools-5.3.3:\n","      Successfully uninstalled cachetools-5.3.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n","google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed appdirs-1.4.4 asgiref-3.8.1 backoff-2.2.1 bcrypt-4.1.3 cachetools-4.2.2 chroma-hnswlib-0.7.3 chromadb-0.5.0 coloredlogs-15.0.1 dataclasses-json-0.6.7 datasets-2.20.0 deprecated-1.2.14 dill-0.3.8 dnspython-2.6.1 email_validator-2.2.0 fastapi-0.111.0 fastapi-cli-0.0.4 h11-0.14.0 httpcore-0.16.3 httptools-0.6.1 httpx-0.23.3 humanfriendly-10.0 importlib-metadata-7.1.0 jsonpatch-1.33 jsonpointer-3.0.0 kubernetes-30.1.0 langchain-0.2.6 langchain-community-0.2.6 langchain-core-0.2.10 langchain-openai-0.1.10 langchain-text-splitters-0.2.2 langchain-zhipu-4.1.5 langsmith-0.1.82 marshmallow-3.21.3 mmh3-4.1.0 monotonic-1.6 multiprocess-0.70.16 mypy-extensions-1.0.0 onnxruntime-1.18.0 openai-1.35.5 opentelemetry-api-1.25.0 opentelemetry-exporter-otlp-proto-common-1.25.0 opentelemetry-exporter-otlp-proto-grpc-1.25.0 opentelemetry-instrumentation-0.46b0 opentelemetry-instrumentation-asgi-0.46b0 opentelemetry-instrumentation-fastapi-0.46b0 opentelemetry-proto-1.25.0 opentelemetry-sdk-1.25.0 opentelemetry-semantic-conventions-0.46b0 opentelemetry-util-http-0.46b0 orjson-3.10.5 overrides-7.7.0 posthog-3.5.0 pyarrow-16.1.0 pyjwt-2.8.0 pypdf-4.2.0 pypika-0.48.9 pysbd-0.3.4 python-dotenv-1.0.1 python-multipart-0.0.9 ragas-0.1.9 requests-2.32.3 rfc3986-1.5.0 starlette-0.37.2 tiktoken-0.7.0 typing-inspect-0.9.0 ujson-5.10.0 uvicorn-0.30.1 uvloop-0.19.0 watchfiles-0.22.0 websockets-12.0 xxhash-3.4.1\n"]}]},{"cell_type":"code","source":["#step1_loader\n","from langchain_community.document_loaders import PyPDFLoader\n","\n","from langchain_zhipu import ZhipuAIEmbeddings\n","from dotenv import load_dotenv\n","\n","load_dotenv()\n","\n","\n","embeddings = ZhipuAIEmbeddings()\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_community.vectorstores import Chroma\n","\n","\n","loader = PyPDFLoader('chatGPT调研报告.pdf')\n","\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=10)\n","\n","docs = loader.load_and_split(text_splitter)\n","print(docs)\n","print(len(docs))\n","\n","chroma = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)\n","\n","chroma.add_documents(docs)\n","print(\"uploaded\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nvO1FmrLVO1e","executionInfo":{"status":"ok","timestamp":1719478102629,"user_tz":-480,"elapsed":75337,"user":{"displayName":"mr li","userId":"02746934671961749824"}},"outputId":"68966a04-71d3-4d45-8237-e2a679f3686f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:pypdf._reader:Ignoring wrong pointing object 46 0 (offset 0)\n"]},{"output_type":"stream","name":"stdout","text":["[Document(page_content='ChatGPT 调研报告\\n（仅供内部参考）\\n哈尔滨工业大学\\n自然语言处理研究所（ HIT-NLP）2023 年 3 月 6 日更多⼲货请关注：公众号：历史的光影', metadata={'source': 'chatGPT调研报告.pdf', 'page': 0}), Document(page_content='ChatGPT 调研报告\\n序言\\n2022年11月30日，OpenAI 推出全新的对话式通用人工智能工具 ——\\nChatGPT 。ChatGPT 表现出了非常惊艳的语言理解、生成、知识推理能力，\\n它可以很好地理解用户意图，做到有效的多轮沟通，并且回答内容完整、重\\n点清晰、有概括、有逻辑、有条理。 ChatGPT 上线后， 5天活跃用户数高达', metadata={'source': 'chatGPT调研报告.pdf', 'page': 1}), Document(page_content='100万，2个月活跃用户数已达 1个亿，成为历史上增长最快的消费者应用\\n程序。除了被广大用户追捧外， ChatGPT 还受到了各国政府、企业界、学\\n术界的广泛关注，使人们看到了解决自然语言处理这一认知智能核心问题的\\n一条可能的路径，并被认为向通用人工智能迈出了坚实的一步，将对搜索引\\n擎构成巨大的挑战，甚至将取代很多人的工作，更将颠覆很多领域和行业。', metadata={'source': 'chatGPT调研报告.pdf', 'page': 1}), Document(page_content='哈工大自然语言处理研究所组织多位老师和同学撰写了本调研报告，从\\n技术原理、应用场景、未来发展等方面对 ChatGPT 进行了尽量详尽的介绍\\n及总结。\\n本报告仅供内部参考。\\n主要编撰人员\\n第一章由车万翔、杨沐昀、张伟男、赵妍妍、冯骁骋、孙承杰、李佳朋编\\n写；第二章由张伟男、隋典伯、高翠芸、朱庆福、李明达、王雪松编写；第\\n三章由刘铭、朱聪慧、汤步洲编写；第四章由徐永东、高翠芸、朱庆福编写；', metadata={'source': 'chatGPT调研报告.pdf', 'page': 1}), Document(page_content='第五章由杨沐昀、张伟男、韩一、庄子彧编写；第六章由隋典伯、高翠芸编\\n写；第七章由车万翔、刘铭编写。参与各章审校工作的还有：崔一鸣、徐志\\n明等。\\n报告整体由车万翔统稿。\\n2更多⼲货请关注：公众号：历史的光影', metadata={'source': 'chatGPT调研报告.pdf', 'page': 1}), Document(page_content='ChatGPT 调研报告\\n目录\\n第一章 ChatGPT 的背景与意义 6\\n1.1自然语言处理的发展历史 .................... 6\\n1.2大规模预训练语言模型的技术发展历程 ............. 8\\n1.3 ChatGPT 技术发展历程 ..................... 8', metadata={'source': 'chatGPT调研报告.pdf', 'page': 2}), Document(page_content='1.3.1 ChatGPT 的相关技术 . . . . . . . . . . . . . . . . . . 10\\n1.3.2 ChatGPT 技术发展脉络的总结 . . . . . . . . . . . . . 11\\n1.3.3 ChatGPT 的未来技术发展方向 . . . . . . . . . . . . . 12', metadata={'source': 'chatGPT调研报告.pdf', 'page': 2}), Document(page_content='1.4 ChatGPT 的优势与劣势 . . . . . . . . . . . . . . . . . . . . . 13\\n1.4.1 ChatGPT 的优势. . . . . . . . . . . . . . . . . . . . . 13\\n1.4.2 ChatGPT 的劣势. . . . . . . . . . . . . . . . . . . . . 15', metadata={'source': 'chatGPT调研报告.pdf', 'page': 2}), Document(page_content='1.5 ChatGPT 的应用前景 . . . . . . . . . . . . . . . . . . . . . . 16\\n1.5.1在人工智能行业的应用前景及影响 . . . . . . . . . . . 17\\n1.5.2在其他行业的应用前景及影响 . . . . . . . . . . . . . . 17', metadata={'source': 'chatGPT调研报告.pdf', 'page': 2}), Document(page_content='1.6 ChatGPT 带来的风险与挑战 . . . . . . . . . . . . . . . . . . 19\\n第二章 ChatGPT 相关核心算法 24\\n2.1基于Transformer 的预训练语言模型 . . . . . . . . . . . . . . 24\\n2.1.1编码预训练语言模型 （ Encoder-only Pre-trained Mod-', metadata={'source': 'chatGPT调研报告.pdf', 'page': 2}), Document(page_content='els）. . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\n2.1.2解码预训练语言模型 （ Decoder-only Pre-trained Mod-\\nels）. . . . . . . . . . . . . . . . . . . . . . . . . . . . 25', metadata={'source': 'chatGPT调研报告.pdf', 'page': 2}), Document(page_content='2.1.3基于编解码架构的预训练语言模型（ Encoder-decoder\\nPre-trained Models ）. . . . . . . . . . . . . . . . . . 28\\n2.2提示学习与指令精调 . . . . . . . . . . . . . . . . . . . . . . . 30', metadata={'source': 'chatGPT调研报告.pdf', 'page': 2}), Document(page_content='2.2.1提示学习概述 . . . . . . . . . . . . . . . . . . . . . . . 30\\n3更多⼲货请关注：公众号：历史的光影', metadata={'source': 'chatGPT调研报告.pdf', 'page': 2}), Document(page_content='ChatGPT 调研报告\\n2.2.2 ChatGPT 中的指令学习 .. . . . . . . . . . . . . . . . 31\\n2.3思维链（ Chain of Thought ，COT）. . . . . . . . . . . . . . 32\\n2.4基于人类反馈的强化学习（ Reinforcement Learning with Hu-', metadata={'source': 'chatGPT调研报告.pdf', 'page': 3}), Document(page_content='man Feedback ，RLHF）. . . . . . . . . . . . . . . . . . . . 33\\n第三章 大模型训练与部署 35\\n3.1大模型并行计算技术 . . . . . . . . . . . . . . . . . . . . . . . 35', metadata={'source': 'chatGPT调研报告.pdf', 'page': 3}), Document(page_content='3.2并行计算框架 . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n3.3模型部署 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\\n3.3.1预训练模型部署的困难 . . . . . . . . . . . . . . . . . . 40', metadata={'source': 'chatGPT调研报告.pdf', 'page': 3}), Document(page_content='3.3.2部署框架和部署工具 . . . . . . . . . . . . . . . . . . . 41\\n3.3.3部署技术和优化方法 . . . . . . . . . . . . . . . . . . . 43\\n3.4预训练模型的压缩 . . . . . . . . . . . . . . . . . . . . . . . . 45', metadata={'source': 'chatGPT调研报告.pdf', 'page': 3}), Document(page_content='3.4.1模型压缩方案概述 . . . . . . . . . . . . . . . . . . . . 45\\n3.4.2结构化模型压缩策略 . . . . . . . . . . . . . . . . . . . 45\\n3.4.3非结构化模型压缩策略 . . . . . . . . . . . . . . . . . . 46', metadata={'source': 'chatGPT调研报告.pdf', 'page': 3}), Document(page_content='3.4.4模型压缩小结 . . . . . . . . . . . . . . . . . . . . . . . 46\\n第四章 ChatGPT 相关数据集 48\\n4.1预训练数据集 . . . . . . . . . . . . . . . . . . . . . . . . . . . 48', metadata={'source': 'chatGPT调研报告.pdf', 'page': 3}), Document(page_content='4.1.1文本预训练数据集 . . . . . . . . . . . . . . . . . . . . 48\\n4.1.2代码预训练数据集 . . . . . . . . . . . . . . . . . . . . 50\\n4.2人工标注数据规范及相关数据集 . . . . . . . . . . . . . . . . . 52', metadata={'source': 'chatGPT调研报告.pdf', 'page': 3}), Document(page_content='4.2.1指令微调工作流程及数据集构建方法 . . . . . . . . . . 53\\n4.2.2常见的指令微调数据集 . . . . . . . . . . . . . . . . . . 53\\n4.2.3构建指令微调数据集的关键问题 . . . . . . . . . . . . . 54\\n第五章 大模型评价方法 59', metadata={'source': 'chatGPT调研报告.pdf', 'page': 3}), Document(page_content='5.1模型评价方式 . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\\n5.1.1人工评价 . . . . . . . . . . . . . . . . . . . . . . . . . 59\\n5.1.2自动评价 . . . . . . . . . . . . . . . . . . . . . . . . . 60', metadata={'source': 'chatGPT调研报告.pdf', 'page': 3}), Document(page_content='5.2模型评价指标 . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\\n5.2.1准确性. . . . . . . . . . . . . . . . . . . . . . . . . . . 62\\n5.2.2不确定性 . . . . . . . . . . . . . . . . . . . . . . . . . 63', metadata={'source': 'chatGPT调研报告.pdf', 'page': 3}), Document(page_content='5.2.3攻击性. . . . . . . . . . . . . . . . . . . . . . . . . . . 63\\n4更多⼲货请关注：公众号：历史的光影', metadata={'source': 'chatGPT调研报告.pdf', 'page': 3}), Document(page_content='ChatGPT 调研报告\\n5.2.4毒害性... . . . . . . . . . . . . . . . . . . . . . . . . 64\\n5.2.5公平性与偏见性 . . . . . . . . . . . . . . . . . . . . . . 65\\n5.2.6鲁棒性. . . . . . . . . . . . . . . . . . . . . . . . . . . 66', metadata={'source': 'chatGPT调研报告.pdf', 'page': 4}), Document(page_content='5.2.7高效性. . . . . . . . . . . . . . . . . . . . . . . . . . . 67\\n5.3模型评价方法小结 . . . . . . . . . . . . . . . . . . . . . . . . 68\\n第六章 现有大模型及对话式通用人工智能系统 69', metadata={'source': 'chatGPT调研报告.pdf', 'page': 4}), Document(page_content='6.1现有大模型对比 . . . . . . . . . . . . . . . . . . . . . . . . . . 69\\n6.2对话式通用人工智能系统调研 . . . . . . . . . . . . . . . . . . 72\\n6.2.1对话式通用人工智能系统 . . . . . . . . . . . . . . . . 72', metadata={'source': 'chatGPT调研报告.pdf', 'page': 4}), Document(page_content='6.2.2不同系统之间的比较 . . . . . . . . . . . . . . . . . . . 75\\n第七章 自然语言处理的未来发展方向 80\\n7.1提高ChatGPT 的能力. . . . . . . . . . . . . . . . . . . . . . 80', metadata={'source': 'chatGPT调研报告.pdf', 'page': 4}), Document(page_content='7.2加深对模型的认识 . . . . . . . . . . . . . . . . . . . . . . . . 81\\n7.3实际应用 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\\n7.4从语言到 AGI的探索之路 . . . . . . . . . . . . . . . . . . . . 83', metadata={'source': 'chatGPT调研报告.pdf', 'page': 4}), Document(page_content='5更多⼲货请关注：公众号：历史的光影', metadata={'source': 'chatGPT调研报告.pdf', 'page': 4}), Document(page_content='ChatGPT 调研报告\\n第一章 ChatGPT 的背景与意义\\n本章首先介绍自然语言处理、大规模预训练语言模型以及 ChatGPT 技\\n术的发展历程，接着就 ChatGPT 的技术优点和不足进行分析，然后讨论\\nChatGPT 可能的应用前景，最后展望 ChatGPT 普及后可能带来的风险与\\n挑战。\\n1.1自然语言处理的发展历史\\n人类语言（又称自然语言）具有无处不在的歧义性、高度的抽象性、近', metadata={'source': 'chatGPT调研报告.pdf', 'page': 5}), Document(page_content='乎无穷的语义组合性和持续的进化性，理解语言往往需要具有一定的知识和\\n推理等认知能力，这些都为计算机处理自然语言带来了巨大的挑战，使其成\\n为机器难以逾越的鸿沟。因此，自然语言处理被认为是目前制约人工智能取\\n得更大突破和更广泛应用的瓶颈之一，又被誉为 “人工智能皇冠上的明珠 ”。\\n国务院2017年印发的《新一代人工智能发展规划》将知识计算与服务、跨', metadata={'source': 'chatGPT调研报告.pdf', 'page': 5}), Document(page_content='媒体分析推理和自然语言处理作为新一代人工智能关键共性技术体系的重\\n要组成部分。\\n自然语言处理自诞生起，经历了五次研究范式的转变（如图 1.1所示） ：\\n由最开始基于小规模专家知识的方法，逐步转向基于机器学习的方法。机器\\n学习方法也由早期基于浅层机器学习的模型变为了基于深度学习的模型。为\\n了解决深度学习模型需要大量标注数据的问题， 2018年开始又全面转向基', metadata={'source': 'chatGPT调研报告.pdf', 'page': 5}), Document(page_content='于大规模预训练语言模型的方法，其突出特点是充分利用 大模型、大数据和\\n大计算以求更好效果。\\n近期，ChatGPT 表现出了非常惊艳的语言理解、生成、知识推理能力，\\n它可以极好地理解用户意图，真正做到多轮沟通，并且回答内容完整、重点\\n清晰、有概括、有逻辑、有条理。 ChatGPT 的成功表现，使人们看到了解\\n决自然语言处理这一认知智能核心问题的一条可能的路径，并被认为向通用', metadata={'source': 'chatGPT调研报告.pdf', 'page': 5}), Document(page_content='人工智能迈出了坚实的一步，将对搜索引擎构成巨大的挑战，甚至将取代很\\n6更多⼲货请关注：公众号：历史的光影', metadata={'source': 'chatGPT调研报告.pdf', 'page': 5}), Document(page_content='ChatGPT调研报告\\n小规模专家知识1950~1990浅层机器学习算法1990~2010深度学习算法2010~2017预训练语言模型2018~2023', metadata={'source': 'chatGPT调研报告.pdf', 'page': 6}), Document(page_content='ChatGPT2023~？图1.1:自然语言处理研究范式的发展历程多人的工作，更将颠覆很多领域和行业。那么，ChatGPT到底解决了什么本质科学问题，才能变得如此强大并受到广泛的关注呢？我们认为，ChatGPT是继数据库和搜索引擎之后的全新一代的“知识表示和调用方式”。知识在计算机内的表示是人工智能的核心问题。如表1.1所示，早期，知识以结构化的方式存储在数据库中，人类需要掌握机器语言（如SQL', metadata={'source': 'chatGPT调研报告.pdf', 'page': 6}), Document(page_content='握机器语言（如SQL），', metadata={'source': 'chatGPT调研报告.pdf', 'page': 6}), Document(page_content='才能调用这些知识；后来，随着互联网的诞生，更多文本、图片、视频等非结构化知识存储在互联网中，人类通过关键词的方式调用搜索引擎获取知识；现在，知识以参数的形式存储在大模型中（从2018年开始） ，ChatGPT主要解决了用自然语言直接调用这些知识的问题，这也是人类获取知识最自然的方式。表1.1:知识表示和调用方式的演进知识表示方式表示方式的精确度知识调用方式调用方式的自然度研究领域 代表应用', metadata={'source': 'chatGPT调研报告.pdf', 'page': 6}), Document(page_content='代表应用 代表公司关系型数据库高SQL低数据库DBMS Oracle、Mi-crosoft互联网 中Keywords中信息检索 搜索引擎Google、Mi-crosoft大模型 低自然语言 高自然语言处理ChatGPT OpenAI、Microsoft、Google另外，从自然语言处理技术发展阶段的角度看（如图1.1）', metadata={'source': 'chatGPT调研报告.pdf', 'page': 6}), Document(page_content='，可以发现一个有趣的现象，即每一个技术阶段的发展时间，大概是上一个阶段的一半。小规模专家知识发展了40年，浅层机器学习是20年，之后深度学习大概10年，预训练语言模型发展的时间是5年，那么以ChatGPT为代表的技7', metadata={'source': 'chatGPT调研报告.pdf', 'page': 6}), Document(page_content='ChatGPT 调研报告\\n术能持续多久呢？如果大胆预测， 可能是2到3年，也就是到 2025年大概\\n又要更新换代了。\\n1.2大规模预训练语言模型的技术发展历程\\n大规模预训练语言模型（简称大模型）作为 ChatGPT 的知识表示及存\\n储基础，对系统效果表现至关重要，接下来对大模型的技术发展历程加以简\\n要介绍。', metadata={'source': 'chatGPT调研报告.pdf', 'page': 7}), Document(page_content='要介绍。\\n2018年，OpenAI 提出了第一代 GPT（Generative Pretrained Trans-\\nformer）模型[1]，将自然语言处理带入 “预训练”时代。然而， GPT模型并没\\n有引起人们的关注，反倒是谷歌随即提出的 BERT（Bidirectional Encoder', metadata={'source': 'chatGPT调研报告.pdf', 'page': 7}), Document(page_content='Representations from Transformers ）模型[2]产生了更大的轰动。不过， Ope-\\nnAI继续沿着初代 GPT的技术思路，陆续发布了 GPT-2[3]和GPT模型\\nGPT-3[4]。\\n尤其是GPT-3模型，含有 1,750亿超大规模参数，并且提出 “提示语”\\n（Prompt）的概念， 只要提供具体任务的提示语， 即便不对模型进行调整也可', metadata={'source': 'chatGPT调研报告.pdf', 'page': 7}), Document(page_content='完成该任务，如：输入 “我太喜欢 ChatGPT 了，这句话的情感是 __”，那\\n么GPT-3就能够直接输出结果 “褒义”。如果在输入中再给一个或几个示例，\\n那么任务完成的效果会更好，这也被称为语境学习（ In-context Learning ）。\\n更详细的技术细节推荐阅读相关的综述文章[5-8]。\\n不过，通过对 GPT-3模型能力的仔细评估发现，大模型并不能真正克', metadata={'source': 'chatGPT调研报告.pdf', 'page': 7}), Document(page_content='服深度学习模型鲁棒性差、可解释性弱、推理能力缺失的问题，在深层次语\\n义理解和生成上与人类认知水平还相去甚远。直到 ChatGPT 的问世，才彻\\n底改变了人们对于大模型的认知。\\n1.3 ChatGPT 技术发展历程\\n2022年11月30日，OpenAI 推出全新的对话式通用人工智能工具——\\nChatGPT 。据报道，在其推出短短几天内，注册用户超过 100万，2个月活', metadata={'source': 'chatGPT调研报告.pdf', 'page': 7}), Document(page_content='跃用户数已达 1个亿，引爆全网热议，成为历史上增长最快的消费者应用程\\n序，掀起了人工智能领域的技术巨浪。\\nChatGPT 之所以有这么多活跃用户，是因为它可以通过学习和理解人\\n类语言，以对话的形式与人类进行交流，交互形式更为自然和精准，极大地\\n改变了普通大众对于聊天机器人的认知，完成了从“人工智障”到“有趣”\\n8更多⼲货请关注：公众号：历史的光影', metadata={'source': 'chatGPT调研报告.pdf', 'page': 7}), Document(page_content='ChatGPT 调研报告\\n的印象转变。除了聊天， ChatGPT 还能够根据用户提出的要求，进行机器\\n翻译、文案撰写、代码撰写等工作。 ChatGPT 拉响了大模型构建的红色警\\n报，学界和企业界纷纷迅速跟进启动研制自己的大模型。\\n继OpenAI 推出ChatGPT 后，与之合作密切的微软迅速上线了基于\\nChatGPT 类技术的 New Bing ，并计划将 ChatGPT 集成到Oﬀice办公套', metadata={'source': 'chatGPT调研报告.pdf', 'page': 8}), Document(page_content='件中。谷歌也迅速行动推出了类似的 Bard与之抗衡。除此之外，苹果、亚\\n马逊、Meta（原Facebook ）等企业也均表示要积极布局 ChatGPT 类技术。\\n国内也有多家企业和机构明确表态正在进行类 ChatGPT 模型研发。百度表\\n示正在基于文心大模型进行文心一言的开发，阿里巴巴表示其类 ChatGPT\\n产品正在研发之中，华为、腾讯表示其在大模型领域均已有相关的布局，网', metadata={'source': 'chatGPT调研报告.pdf', 'page': 8}), Document(page_content='易表示其已经投入到类 ChatGPT 技术在教育场景的落地研发，京东表示将\\n推出产业版 ChatGPT ，科大讯飞表示将在数月后进行产品级发布，国内高\\n校复旦大学则推出了类 ChatGPT 的MOSS模型。\\n除了国内外学界和企业界在迅速跟进以外，我国国家层面也对 Chat-\\nGPT有所关注。 2023年2月24日， 科技部部长王志刚表示： “ ChatGPT 在', metadata={'source': 'chatGPT调研报告.pdf', 'page': 8}), Document(page_content='自然语言理解、自然语言处理等方面有进步的地方，同时在算法、数据、算\\n力上进行了有效结合。 ”科技部高新技术司司长陈家昌在回应 ChatGPT 相\\n关提问时也表示， ChatGPT 最近形成了一种现象级的应用，表现出很高的\\n人机交互水平，表现出自然语言的大模型已经具备了面向通用人工智能的一\\n些特征，在众多行业领域有着广泛的应用潜力。1', metadata={'source': 'chatGPT调研报告.pdf', 'page': 8}), Document(page_content='ChatGPT 是现象级应用，标志着语言大模型已经具备了一些通用人工\\n智能特征， 在众多行业领域有着广泛的应用潜力。 ” 这标志着在未来， ChatGPT\\n相关技术有可能会成为国家战略支持的重点。\\n从技术角度讲， ChatGPT 是一个聚焦于对话生成的大语言模型，其能\\n够根据用户的文本描述，结合历史对话，产生相应的智能回复。其中 GPT', metadata={'source': 'chatGPT调研报告.pdf', 'page': 8}), Document(page_content='是英文Generative Pretrained Transformer 的缩写。 GPT通过学习大量网\\n络已有文本数据（如 Wikipedia ，reddit对话） ，获得了像人类一样流畅对话\\n的能力。虽然 GPT可以生成流畅的回复，但是有时候生成的回复并不符合\\n人类的预期， OpenAI 认为符合人类预期的回复应该具有真实性、无害性和', metadata={'source': 'chatGPT调研报告.pdf', 'page': 8}), Document(page_content='有用性。为了使生成的回复具有以上特征， OpenAI 在2022年初发表的工\\n作“Training language models to follow instructions with human feedback ”\\n中提到引入人工反馈机制，并使用近端策略梯度算法（ PPO）对大模型进行\\n1https://www.sohu.com/a/645545405_120109837', metadata={'source': 'chatGPT调研报告.pdf', 'page': 8}), Document(page_content='9更多⼲货请关注：公众号：历史的光影', metadata={'source': 'chatGPT调研报告.pdf', 'page': 8}), Document(page_content='ChatGPT 调研报告\\n训练。这种基于人工反馈的训练模式能够很大程度上减小大模型生成回复与\\n人类回复之间的偏差，也使得 ChatGPT 具有良好的表现。\\n1.3.1ChatGPT 的相关技术\\n接下来将简要介绍 ChatGPT 相关技术的发展历程。 ChatGPT 核心技\\n术主要包括其具有良好的自然语言生成能力的大模型 GPT-3.5 以及训练这', metadata={'source': 'chatGPT调研报告.pdf', 'page': 9}), Document(page_content='一模型的钥匙——基于人工反馈的强化学习（ RLHF）。\\nGPT家族是OpenAI 公司推出的相关产品，这是一种生成式语言模型，\\n可用于对话、问答、机器翻译、写代码等一系列自然语言任务。每一代 GPT\\n相较于上一代模型的参数量均呈现出爆炸式增长。 OpenAI 在2018年6月\\n发布的GPT包含1.2亿参数，在 2019年2月发布的 GPT-2包含15亿参', metadata={'source': 'chatGPT调研报告.pdf', 'page': 9}), Document(page_content='数，在2020年5月发布的 GPT-3包含1750亿参数。与相应参数量一同增\\n长的还有公司逐年积淀下来的恐怖的数据量。可以说大规模的参数与海量的\\n训练数据为 GPT系列模型赋能，使其可以存储海量的知识、理解人类的自\\n然语言并且有着良好的表达能力。\\n除了参数上的增长变化之外， GPT模型家族的发展从 GPT-3开始分\\n成了两个技术路径并行发展2，一个路径是以 Codex为代表的代码预训练', metadata={'source': 'chatGPT调研报告.pdf', 'page': 9}), Document(page_content='技术，另一个路径是以 InstructGPT 为代表的文本指令（ Instruction ）预\\n训练技术。但这两个技术路径不是始终并行发展的，而是到了一定阶段后\\n（具体时间不详）进入了融合式预训练的过程，并通过指令学习（ Instruction\\nTuning）、 有 监 督 精 调 （ Supervised Fine-tuning ）以及基于人类反馈的强化', metadata={'source': 'chatGPT调研报告.pdf', 'page': 9}), Document(page_content='学习（Reinforcement Learning with Human Feedback ，RLHF）等技术实现\\n了以自然语言对话为接口的 ChatGPT 模型。\\nRLHF这一概念最早是在 2008年TAMER：Training an Agent Man-\\nually via Evaluative Reinforcement[9]一文中被提及的。在传统的强化学习', metadata={'source': 'chatGPT调研报告.pdf', 'page': 9}), Document(page_content='框架下代理 (Agent) 提供动作给环境，环境输出奖励和状态给代理，而在\\nTAMER 框架下，引入人类标注人员作为系统的额外奖励。该文章中指出引\\n入人类进行评价的主要目的是加快模型收敛速度，降低训练成本，优化收敛\\n方向。具体实现上，人类标注人员扮演用户和代理进行对话，产生对话样本\\n并对回复进行排名打分，将更好的结果反馈给模型，让模型从两种反馈模式', metadata={'source': 'chatGPT调研报告.pdf', 'page': 9}), Document(page_content='——人类评价奖励和环境奖励中学习策略，对模型进行持续迭代式微调。这\\n一框架的提出成为后续基于 RLHF相关工作的理论基础。\\n2https://openai.com/blog/\\n10更多⼲货请关注：公众号：历史的光影', metadata={'source': 'chatGPT调研报告.pdf', 'page': 9}), Document(page_content='ChatGPT 调研报告\\n在2017年前后，深度强化学习（ Deep Reinforcement Learning ）逐渐\\n发展并流行起来。 MacGlashan et al.[10]提出了一种 AC算法（Actor-critic ），\\n并且将人工反馈（包括积极和消极）作为信号调节优势函数（ Advantage', metadata={'source': 'chatGPT调研报告.pdf', 'page': 10}), Document(page_content='function）。Warnell et al.[11]将TAMER 框架与深度强化学习相结合，成功\\n将RLHF引入深度强化学习领域。在这一阶段， RLHF主要被应用于模拟\\n器环境（例如游戏等）或者现实环境（例如机器人等）领域，而利用其对于\\n语言模型进行训练并未受到重视。\\n在2019年以后，RLHF与语言模型相结合的工作开始陆续出现， Ziegler', metadata={'source': 'chatGPT调研报告.pdf', 'page': 10}), Document(page_content='et al.[12]较早利用人工信号在四个具体任务上进行了微调并取得不错的效果。\\nOpenAI 从2020年开始关注这一方向并陆续发表了一系列相关工作，如应\\n用于文本摘要[13-14]，利用RLHF训练一个可以进行网页导航的代理[15]等。\\n后来，OpenAI 将RLHF与GPT相结合的工作，提出了 InstructGPT 这', metadata={'source': 'chatGPT调研报告.pdf', 'page': 10}), Document(page_content='一ChatGPT 的孪生兄弟[16]，主要是利用 GPT-3进行对话生成，旨在改善\\n模型生成的真实性、无害性和有用性。与此同时，作为缔造 AlphaGo 的公\\n司，具有一干擅长强化学习的算法工程师的 DeepMind 也关注到了这一方\\n向，先后发表了 GopherCite[17]和Sparrow[18]两个利用 RLHF进行训练的语', metadata={'source': 'chatGPT调研报告.pdf', 'page': 10}), Document(page_content='言模型， GopherCite 是在开放域问答领域的工作， Sparrow 是在对话领域的\\n一篇工作，并且在 2022年9月，DeepMind 的聊天机器人也已经上线。\\n2022年12月，OpenAI 在诸多前人工作的积淀之下推出了 ChatGPT 。\\nChatGPT 以GPT-3.5 作为基座，依托其强大的生成能力，使用 RLHF对\\n其进行进一步训练，从而取得了惊艳四座的效果。', metadata={'source': 'chatGPT调研报告.pdf', 'page': 10}), Document(page_content='1.3.2 ChatGPT 技术发展脉络的总结\\n纵观ChatGPT 的发展历程，不难发现其成功是循序渐进的， OpenAI\\n从2020年开始关注 RLHF这一研究方向，并且开展了大量的研究工作，积\\n攒了足够的强化学习在文本生成领域训练的经验。 GPT系列工作的研究则\\n积累了海量的训练数据以及大语言模型训练经验，这两者的结合才产生了', metadata={'source': 'chatGPT调研报告.pdf', 'page': 10}), Document(page_content='ChatGPT 。可以看出技术的发展并不是一蹴而就的，是大量工作的积淀量\\n变引起质变。此外，将 RLHF这一原本应用于模拟器环境和现实环境下的\\n强化学习技术迁移到自然语言生成任务上是其技术突破的关键点之一。\\n纵观AI这几年的发展，已经逐渐呈现出不同技术相互融合的大趋势，\\n比如将Transformer 引入计算机视觉领域产生的 ViT；将强化学习引入蛋白', metadata={'source': 'chatGPT调研报告.pdf', 'page': 10}), Document(page_content='质结构预测的 AlphaFold 等。每个研究人员都有自己熟悉擅长的领域，而同\\n11更多⼲货请关注：公众号：历史的光影', metadata={'source': 'chatGPT调研报告.pdf', 'page': 10}), Document(page_content='ChatGPT 调研报告\\n时科学界也存在着大量需要 AI赋能的亟待解决的关键问题，如何发现这些\\n问题的痛点，设计合理的方法，利用自己研究领域的优越的技术解决问题，\\n似乎是一个值得思考，也非常有意义的问题。\\n这是一个 AI蓬勃发展的时代，计算机科学界每天都在产生着令人惊奇\\n的发明创造，很多之前人们可望而不可及的问题都在或者正在被解决的路', metadata={'source': 'chatGPT调研报告.pdf', 'page': 11}), Document(page_content='上。2022年2月，DeepMind 发布可对托卡马克装置中等离子体进行磁控制\\n的以帮助可控核聚变的人工智能，这项研究目前仍在进行。或许在未来的某\\n一天，能源将不成为困扰我们的问题，环境污染将大大减少，星际远航将成\\n为可能。希望每个研究人员都能在这样的时代中，找到适合自己的研究方向\\n并且为科技进步添砖加瓦。\\n1.3.3 ChatGPT 的未来技术发展方向', metadata={'source': 'chatGPT调研报告.pdf', 'page': 11}), Document(page_content='虽然ChatGPT 目前已经取得了非常喜人的成果，但是未来仍然有诸多\\n可以研究的方向。\\n首先OpenAI 的研究人员指出了 ChatGPT 现存的一些问题：\\n1. ChatGPT 有时候会生成一些似是而非、毫无意义的答案，导致这个问\\n题的原因有：强化学习训练过程中没有明确的正确答案；训练过程中\\n一些谨慎的训练策略导致模型无法产生本应产生的正确回复；监督学', metadata={'source': 'chatGPT调研报告.pdf', 'page': 11}), Document(page_content='习训练过程中错误的引导导致模型更倾向于生成标注人员所知道的内\\n容而不是模型真实知道的。\\n2. ChatGPT 对于输入措辞比较敏感，例如：给定一个特定的问题，模型\\n声称不知道答案，但只要稍微改变措辞就可以生成正确答案。\\n3. ChatGPT 生成的回复通常过于冗长， 并且存在过度使用某些短语的问\\n题，例如：重申是由 OpenAI 训练的语言模型。这样的问题主要来自\\n于训练数据的偏差和过拟合问题。', metadata={'source': 'chatGPT调研报告.pdf', 'page': 11}), Document(page_content='4.虽然OpenAI 已经努力让模型拒绝不恰当和有害的请求，但是仍然无\\n法避免对有害请求作出回复或对问题表现出偏见。\\n其次，ChatGPT 虽然很强大，但是其模型过于庞大使用成本过高，如\\n何对模型进行瘦身也是一个未来的发展方向，目前主流的模型压缩方法有量\\n化、剪枝、蒸馏和稀疏化等。量化是指降低模型参数的数值表示精度，比如\\n从FP32降低到FP16或者INT8。剪枝是指合理地利用策略删除神经网络', metadata={'source': 'chatGPT调研报告.pdf', 'page': 11}), Document(page_content='12更多⼲货请关注：公众号：历史的光影', metadata={'source': 'chatGPT调研报告.pdf', 'page': 11}), Document(page_content='ChatGPT 调研报告\\n中的部分参数，比如从单个权重到更高粒度组件如权重矩阵到通道， 这种方\\n法在视觉领域或其他较小语言模型中比较奏效。蒸馏是指利用一个较小的学\\n生模型去学习较大的老师模型中的重要信息而摒弃一些冗余信息的方法。稀\\n疏化将大量的冗余变量去除，简化模型的同时保留数据中最重要的信息。\\n此外，减少人类反馈信息的 RLAIF也是最近被提出的一个全新的观点。', metadata={'source': 'chatGPT调研报告.pdf', 'page': 12}), Document(page_content='2022年12月Anthropic 公司发表论文“ Constitutional AI: Harmlessness\\nfrom AI Feedback ”[19]，该公司是 2020年OpenAI 副总裁离职后创立的，其\\n公司始创团队中多有参与 GPT-3以及RLHF相关研究的经历。该文章介绍\\n了其最新推出的聊天机器人 Claude，与ChatGPT 类似的是两者均利用强', metadata={'source': 'chatGPT调研报告.pdf', 'page': 12}), Document(page_content='化学习对模型进行训练，而不同点则在于其排序过程使用模型进行数据标注\\n而非人类，即训练一个模型学习人类对于无害性偏好的打分模式并代替人类\\n对结果进行排序。\\n1.4 ChatGPT 的优势与劣势\\n1.4.1 ChatGPT 的优势\\nChatGPT 作为开年爆款产品，自发布以来不足三个月，就以其能力的\\n全面性、回答的准确性、生成的流畅性、丰富的可玩性俘获了数以亿计的', metadata={'source': 'chatGPT调研报告.pdf', 'page': 12}), Document(page_content='用户，其整体能力之强大令人惊叹。下面我们将从以下三个角度分别阐述\\nChatGPT 相较于不同产品和范式的优点。\\n1.相较于普通聊天机器人： ChatGPT 的发布形式是一款聊天机器人，类\\n似于市场上其他聊天机器人（微软小冰、百度度秘等） ，也是直接对其下指\\n令即可与人类自然交互，简单直接。但相较之下， ChatGPT 的回答更准确，', metadata={'source': 'chatGPT调研报告.pdf', 'page': 12}), Document(page_content='答案更流畅，能进行更细致的推理，能完成更多的任务，这得益于其以下三\\n方面的能力：\\n1.强大的底座能力： ChatGPT 基于GPT-3.5 系列的Code-davinci-002\\n指令微调而成。而 GPT-3.5 系列是一系列采用了数千亿的 token预训\\n练的千亿大模型，足够大的模型规模赋予了 ChatGPT 更多的参数量\\n记忆充足的知识，同时其内含“涌现”的潜力，为之后的指令微调能', metadata={'source': 'chatGPT调研报告.pdf', 'page': 12}), Document(page_content='力激发打下了坚实的基础；\\n2.惊艳的思维链推理能力：在文本预训练的基础上， ChatGPT 的基础大\\n模型采用 159G的代码进行了继续预训练，借助代码分步骤、分模块\\n13更多⼲货请关注：公众号：历史的光影', metadata={'source': 'chatGPT调研报告.pdf', 'page': 12}), Document(page_content='ChatGPT 调研报告\\n解决问题的特性，模型涌现出了逐步推理的能力，在模型表现上不再\\n是随着模型规模线性增长，有了激增，打破了 scaling law ；\\n3.实用的零样本能力： ChatGPT 通过在基础大模型上利用大量种类的\\n指令进行指令微调，模型的泛化性得到了显著地激发，可以处理未见\\n过的任务，使其通用性大大提高，在多种语言、多项任务上都可以进\\n行处理。', metadata={'source': 'chatGPT调研报告.pdf', 'page': 13}), Document(page_content='行处理。\\n综上，在大规模语言模型存储充足的知识和涌现的思维链能力的基础\\n上，ChatGPT 辅以指令微调，几乎做到了知识范围内的无所不知，且难以\\n看出破绽，已遥遥领先普通的聊天机器人。\\n2.相较于其它大规模语言模型： 相较于其它的大规模语言模型， ChatGPT\\n使用了更多的多轮对话数据进行指令微调，这使其拥有了建模对话历史的能\\n力，能持续和用户交互。', metadata={'source': 'chatGPT调研报告.pdf', 'page': 13}), Document(page_content='同时因为现实世界语言数据的偏见性，大规模语言模型基于这些数据预\\n训练可能会生成有害的回复。 ChatGPT 在指令微调阶段通过基于人类反馈\\n的强化学习调整模型的输出偏好，使其能输出更符合人类预期的结果（即能\\n进行翔实的回应、公平的回应、拒绝不当问题、拒绝知识范围外的问题） ，一\\n定程度上缓解了安全性和偏见问题，使其更加耐用；同时其能利用真实的用', metadata={'source': 'chatGPT调研报告.pdf', 'page': 13}), Document(page_content='户反馈不断进行 AI正循环，持续增强自身和人类的这种对齐能力，输出更\\n安全的回复。\\n3.相较于微调小模型： 在ChatGPT 之前，利用特定任务数据微调小模\\n型是近年来最常用的自然语言处理范式。相较于这种微调范式， ChatGPT\\n通过大量指令激发的泛化能力在零样本和少样本场景下具有显著优势，在未\\n见过的任务上也可以有所表现。例如 ChatGPT 的前身InstructGPT 指令', metadata={'source': 'chatGPT调研报告.pdf', 'page': 13}), Document(page_content='微调的指令集中 96%以上是英语，此外只含有 20种少量的其它语言（包含\\n西班牙语、法语、德语等） 。然而在机器翻译任务上，我们使用指令集中未出\\n现的塞尔维亚语让 ChatGPT 进行翻译，仍然可以得到正确的翻译结果，这\\n是在微调小模型的范式下很难实现的泛化能力。\\n除此之外，作为大规模语言模型的天然优势使 ChatGPT 在创作型任务\\n上的表现尤为突出，甚至强于大多数普通人类。', metadata={'source': 'chatGPT调研报告.pdf', 'page': 13}), Document(page_content='14更多⼲货请关注：公众号：历史的光影', metadata={'source': 'chatGPT调研报告.pdf', 'page': 13}), Document(page_content='ChatGPT 调研报告\\n1.4.2 ChatGPT 的劣势\\n固然ChatGPT 在实际使用中表现惊艳， 然而囿于大规模语言模型自身、\\n数据原因、标注策略等局限，仍主要存在以下劣势：\\n1.大规模语言模型自身的局限： 身为大规模语言模型， ChatGPT 难免有\\n着LLM的通用局限，具体表现在以下几个方面：\\n1.可信性无法保证： ChatGPT 的回复可能是在一本正经地胡说八道，语', metadata={'source': 'chatGPT调研报告.pdf', 'page': 14}), Document(page_content='句通畅貌似合理，但其实完全大相径庭，目前模型还不能提供合理的\\n证据进行可信性的验证；\\n2.时效性差： ChatGPT 无法实时地融入新知识，其知识范围局限于基础\\n大规模语言模型使用的预训练数据时间之前，可回答的知识范围有明\\n显的边界；\\n3.成本高昂： ChatGPT 基础大模型训练成本高、部署困难、每次调用花\\n费不菲、还可能有延迟问题，对工程能力有很高的要求；', metadata={'source': 'chatGPT调研报告.pdf', 'page': 14}), Document(page_content='4.在特定的专业领域上表现欠佳：大规模语言模型的训练数据是通用数\\n据，没有领域专业数据，比如针对特定领域的专业术语翻译做的并不\\n好；\\n5.语言模型每次的生成结果是 beam search 或者采样的产物，每次都会\\n有细微的不同。同样地， ChatGPT 对输入敏感，对于某个指令可能回\\n答不正确，但稍微替换几个词表达同样的意思重新提问，又可以回答\\n正确，目前还不够稳定。', metadata={'source': 'chatGPT调研报告.pdf', 'page': 14}), Document(page_content='2.数据原因导致的局限： 如上文所述， ChatGPT 的基础大规模语言模型\\n是基于现实世界的语言数据预训练而成，因为数据的偏见性，很可能生成有\\n害内容。虽然 ChatGPT 已采用RLHF的方式大大缓解了这一问题，然而\\n通过一些诱导，有害内容仍有可能出现。\\n此外，ChatGPT 为OpenAI 部署，用户数据都为 OpenAI 所掌握，长\\n期大规模使用可能存在一定的数据泄漏风险。', metadata={'source': 'chatGPT调研报告.pdf', 'page': 14}), Document(page_content='3.标注策略导致的局限： ChatGPT 通过基于人类反馈的强化学习使模型\\n的生成结果更符合人类预期，然而这也导致了模型的行为和偏好一定程度上\\n15更多⼲货请关注：公众号：历史的光影', metadata={'source': 'chatGPT调研报告.pdf', 'page': 14}), Document(page_content='ChatGPT 调研报告\\n反映的是标注人员的偏好，在标注人员分布不均的情况下， 可能会引入新的\\n偏见问题。同样地，标注人员标注时会倾向于更长的答案，因为这样的答案\\n看起来更加全面，这导致了 ChatGPT 偏好于生成更长的回答，在部分情况\\n下显得啰嗦冗长。\\n此外，作为突围型产品， ChatGPT 确实表现优秀。然而在目前微调小', metadata={'source': 'chatGPT调研报告.pdf', 'page': 15}), Document(page_content='模型已经达到较好效果的前提下，同时考虑到 ChatGPT 的训练和部署困难\\n程度，ChatGPT 可能在以下任务场景下不太适用或者相比于目前的微调小\\n模型范式性价比较低：\\n1. ChatGPT 的通用性很强，对多种自然语言处理任务都有处理能力。然\\n而针对特定的序列标注等传统自然语言理解任务，考虑到部署成本和\\n特定任务的准确性，在 NLU任务不需要大规模语言模型的生成能力，', metadata={'source': 'chatGPT调研报告.pdf', 'page': 15}), Document(page_content='也不需要更多额外知识的前提下，如果拥有足够数据进行微调，微调\\n小模型可能仍是更佳的方案；\\n2.在一些不需要大规模语言模型中额外知识的任务上，例如机器阅读理\\n解，回答问题所需的知识已经都存在于上下文中；\\n3.由于除英语之外的其它语言在预训练语料库中占比很少，因此翻译目\\n标非英文的机器翻译任务和多语言任务在追求准确的前提下可能并不\\n适用；', metadata={'source': 'chatGPT调研报告.pdf', 'page': 15}), Document(page_content='适用；\\n4.大规模语言模型的现实世界先验知识太强，很难被提示覆盖，这导致\\n我们很难纠正 ChatGPT 的事实性错误，使其使用场景受限；\\n5.对于常识、符号和逻辑推理问题， ChatGPT 更倾向于生成“不确定”\\n的回复，避免直接面对问题正面回答。在追求唯一性答案的情况下可\\n能并不适用；\\n6. ChatGPT 目前还只能处理文本数据，在多模态任务上还无法处理。', metadata={'source': 'chatGPT调研报告.pdf', 'page': 15}), Document(page_content='表1.2列举了一些 ChatGPT 存在的以上不足的示例（ 2023年2月24\\n日测试） 。\\n1.5 ChatGPT 的应用前景\\nChatGPT 作为掀起新一轮 AIGC热潮的新引擎，无论在人工智能行业\\n还是其他行业都带来了广泛的讨论和影响，下面我们分别从这两个方面讨论\\n16更多⼲货请关注：公众号：历史的光影', metadata={'source': 'chatGPT调研报告.pdf', 'page': 15}), Document(page_content='ChatGPT 调研报告\\nChatGPT 的应用前景。\\n1.5.1在人工智能行业的应用前景及影响\\nChatGPT 的发布及其取得的巨大成功对人工智能行业形成了强烈的冲\\n击，人们发现之前许多悬而未解的问题在 ChatGPT 身上迎刃而解（包括事\\n实型问答、 文本摘要事实一致性、 篇章级机器翻译的性别问题等） ， ChatGPT', metadata={'source': 'chatGPT调研报告.pdf', 'page': 16}), Document(page_content='引起了巨大的恐慌。然而从另一个角度看，我们也可以把 ChatGPT 当成是\\n一个工具来帮助我们的开发、优化我们的模型、丰富我们的应用场景，比如：\\n1.代码开发 ：利用ChatGPT 辅助开发代码，提高开发效率，包括代码\\n补全、自然语言指令生成代码、代码翻译、 bug修复等；\\n2.ChatGPT 和具体任务相结合 ：ChatGPT 的生成结果在许多任务上', metadata={'source': 'chatGPT调研报告.pdf', 'page': 16}), Document(page_content='相比微调小模型都有很明显的可取之处（比如文本摘要的事实一致\\n性，篇章级机器翻译的性别问题） ，在微调小模型的基础上结合这些\\nChatGPT 的长处，可能可以在避免训练部署下显著提升小模型的效\\n果；\\n3.同时基于 ChatGPT 指令微调激发的零样本能力，对于只有少数标注\\n或者没有标注数据的任务以及需要分布外泛化的任务，我们既可以直', metadata={'source': 'chatGPT调研报告.pdf', 'page': 16}), Document(page_content='接应用ChatGPT ，也可以把 ChatGPT 当作冷启动收集相关语料的工\\n具，丰富我们的应用场景。\\n1.5.2在其他行业的应用前景及影响\\nChatGPT 的发布也引起了其它行业的连锁反应： Stack Overflow 禁用\\nChatGPT 的生成内容，美国多所公立学校禁用 ChatGPT ，各大期刊禁止将\\nChatGPT 列为合著者。 ChatGPT 似乎在一些行业成为“公敌” ，但在其它', metadata={'source': 'chatGPT调研报告.pdf', 'page': 16}), Document(page_content='行业，也许充满机遇。\\n1.搜索引擎 ：自ChatGPT 发布以来，各大科技巨头都投入了极大的关\\n注度，最著名的新闻莫过于谷歌担心 ChatGPT 会打破搜索引擎的使\\n用方式和市场格局而拉响的红色警报。为此各大科技巨头纷纷行动起\\n来，谷歌开始内测自己的类 ChatGPT 产品Bard，百度三月份将面向\\n公众开放文心一言， 微软更是宣布 ChatGPT 为必应提供技术支持， 推', metadata={'source': 'chatGPT调研报告.pdf', 'page': 16}), Document(page_content='出新必应。 ChatGPT 和搜索引擎的结合似乎已经不可避免，也许不会\\n17更多⼲货请关注：公众号：历史的光影', metadata={'source': 'chatGPT调研报告.pdf', 'page': 16}), Document(page_content='ChatGPT 调研报告\\n马上取代搜索引擎，但基于搜索引擎为 ChatGPT 提供生成结果证据\\n展示以及利用检索的新知识扩展 ChatGPT 的回答边界已经是可以预\\n见并正在进行的结合方向。\\n2.泛娱乐行业 ：ChatGPT 对于文娱行业则更多带来的是机遇。无论是基\\n于ChatGPT 创建更智能的游戏虚拟人和玩家交流提升体验，还是利', metadata={'source': 'chatGPT调研报告.pdf', 'page': 17}), Document(page_content='用虚拟数字人进行虚拟主播直播互动， ChatGPT 都为类似的数字人提\\n供了更智能的“大脑” ，使行业充满想象空间。除此之外，在心理健康\\n抚慰、闲聊家庭陪护等方面，类似的数字人也大有拳脚可展。\\n3.自媒体行业 ：同样大大受益的还有自媒体行业。美国的新闻聚合网站\\nBuzzFeed 宣布和OpenAI 合作，未来将使用 ChatGPT 帮助创作内', metadata={'source': 'chatGPT调研报告.pdf', 'page': 17}), Document(page_content='容。ChatGPT 的出现将使得内容创作变得更加容易，无论是旅游、餐\\n饮、住宿、情感，相关博主的内容产出效率将得到极大的提升，有更多\\n的精力润色相关内容，期待更多的高质量文章的产生。\\n4.教育行业 ：ChatGPT 在教育行业可能是彻头彻尾的“大魔王” ：调查显\\n示89%的学生利用 ChatGPT 完成家庭作业，世界宗教课全班第一的', metadata={'source': 'chatGPT调研报告.pdf', 'page': 17}), Document(page_content='论文竟然是用 ChatGPT 所写。这迫使多所学校全面禁用 ChatGPT ，\\n无论是在作业、考试或者论文当中，一经发现即认定为作弊。然而从另\\n一方面来看，这可能也会促使针对人工智能相关法律法规的完善，加\\n速AI社会化的发展。\\n5.其他专业领域 ：针对其它专业领域， ChatGPT 的具体影响不大。因为\\n限于ChatGPT 训练数据的限制， ChatGPT 无法对专业领域的专业', metadata={'source': 'chatGPT调研报告.pdf', 'page': 17}), Document(page_content='知识进行细致的分析，生成的回答专业度不足且可信性难以保证，至\\n多只能作为参考，很难实现替代。比如因为 ChatGPT 未获取IDC、\\nGartner 等机构的数据使用授权，其关于半导体产业的市场分析中很\\n少涉及量化的数据信息。\\n此外，ChatGPT 可以帮助个人使用者在日常工作中写邮件、 演讲稿、 文\\n案和报告，提高其工作效率。同时基于微软计划将 ChatGPT 整合进Word、', metadata={'source': 'chatGPT调研报告.pdf', 'page': 17}), Document(page_content='PowerPoint 等办公软件，个人使用者也可以从中受益，提高办公效率。\\n18更多⼲货请关注：公众号：历史的光影', metadata={'source': 'chatGPT调研报告.pdf', 'page': 17}), Document(page_content='ChatGPT 调研报告\\n1.6 ChatGPT 带来的风险与挑战\\nChatGPT 的出现和应用给用户和社会带来了很多新的风险和挑战。这\\n些风险和挑战，一部分是 ChatGPT 本身技术限制引起的，如生成的内容不\\n能保证真实性、会产生有害言论等。一部分是用户对 ChatGPT 的使用不当\\n引起的，如在教育、科研等领域滥用 ChatGPT 产生的文本。 ChatGPT 用', metadata={'source': 'chatGPT调研报告.pdf', 'page': 18}), Document(page_content='户数量在其出现后两个月就突破了 1亿，因此应对这些风险和挑战需要整个\\n社会行动起来，制定相应的法律和规范，让 ChatGPT 为人类发展服务，尽\\n量避免引起新的的社会问题。下面列举了几个重要风险和挑战，并试着给出\\n了相应的解决思路。\\n滥用风险 滥用风险主要是指用户对于 ChatGPT 产生结果的不当应用。具\\n体表现有：学生在课堂测验或考试过程中直接使用 ChatGPT 的结果作为答', metadata={'source': 'chatGPT调研报告.pdf', 'page': 18}), Document(page_content='案进行作弊；研究人员使用 ChatGPT 来进行写作的学术不规范行为；不法\\n分子利用 ChatGPT 来制造假新闻或谣言。 Tamkin et al.[20]指出，使用预训\\n练语言模型能参与的犯罪行为种类繁多，因此很难把所有它们能错误使用的\\n方法都归纳总结起来，可以预料随着技术的发展以及不法分子的不断尝试，\\nChatGPT 被错误使用的方式会更多样且更加难以预测。', metadata={'source': 'chatGPT调研报告.pdf', 'page': 18}), Document(page_content='已有很多研究者针对这一需求提出了不同的解决方案。下面主要介绍两\\n个有代表性的工作：\\n2023年1月31日，开发 ChatGPT 的OpenAI 公司发布了一个能够鉴\\n别AI生成文本的分类器3。根据OpenAI 公布的测试结果，该分类器对于\\n“AI生成文本”类别的召回率只有 26%。该分类器的训练数据的构造方式如\\n下：首先获取大量提示，对于每个提示，分别获取 AI生成文本和人工写作', metadata={'source': 'chatGPT调研报告.pdf', 'page': 18}), Document(page_content='文本。这种训练数据的获取方式成本较高。\\n斯坦福大学的 Mitchell et al.[21]提出了一种 Zero-shot 的AI生成文本\\n检测方法 DetectGPT ，该方法利用 AI生成文本和人工写作文本在由其他\\nAI模型进行改写后所引起的生成概率的变化来进行判别，生成概率变化大\\n的文本为 AI生成文本。根据论文在 3个数据集上的测试结果， DetectGPT', metadata={'source': 'chatGPT调研报告.pdf', 'page': 18}), Document(page_content='在AUROC 这一评价指标上超过了目前已知的其他 Zero-shot 方法。Detect-\\nGPT的优势是不需要训练数据，但是它需要能够输出生成概率的 AI模型\\n的支持，而很多 AI模型只提供了 API（如GPT-3）， 无 法 计 算 生 成 文 本 的\\n概率。', metadata={'source': 'chatGPT调研报告.pdf', 'page': 18}), Document(page_content='概率。\\n3https://openai.com/blog/new-ai- classifier-for-indicating-ai-written-text/\\n19更多⼲货请关注：公众号：历史的光影', metadata={'source': 'chatGPT调研报告.pdf', 'page': 18}), Document(page_content='ChatGPT 调研报告\\n总的来说，目前对于 ChatGPT 自动生成文本的自动鉴别技术效果还不\\n能令人满意，需要继续寻找更有效的鉴别方法。\\n错误信息风险 错误信息风险源于 ChatGPT 可能产生虚假、 误导、无意义\\n或质量差的信息。 ChatGPT 可以并且已经在成为很多用户的一种获取信息\\n的手段，但用户如果没有分辨能力，可能会采信这些错误信息，从而带来风', metadata={'source': 'chatGPT调研报告.pdf', 'page': 19}), Document(page_content='险隐患。尽管预训练语言模型生成的信息有一定可信度，且可信度会在后\\n续学习改进中不断上升[15]，但这类模型在很多领域生成的信息仍然不够可\\n靠[22]，ChatGPT 也是如此。 ChatGPT 的流行会在某种程度上增加用户对\\n它的信任，从而被更多错误的信息误导。预训练语言模型的生成的错误信息\\n比例上升可能会加大人们对社会中各类信息的不信任，破坏社会的知识交流\\n传播[23]。', metadata={'source': 'chatGPT调研报告.pdf', 'page': 19}), Document(page_content='传播[23]。\\n在一些很敏感的领域， 比如法律和医学， ChatGPT 的错误信息很容易导\\n致直接伤害。错误的医学法律知识会导致使用者违法犯罪或者自行处理伤口\\n疾病时出现问题，从而造成对社会和自己身体健康的伤害。这在 ChatGPT\\n之前就已经有了一些例子，如患者不相信正规医生而搬出搜索引擎给出的结\\n果来反驳医生，这也能体现出很多用户对这类信息获取方式的信任。', metadata={'source': 'chatGPT调研报告.pdf', 'page': 19}), Document(page_content='知识共享是一种社会现象，人们出于信任从社会中获取知识并且过滤吸\\n收。ChatGPT 的一个较为常用的功能是充当搜索引擎，类似百度、 Google\\n等，搜索引擎的信息因其较高的准确率通常拥有较高的可信度，但是如果\\nChatGPT 产生错误信息误导他人的现象加剧可能会导致人们不仅对 Chat-\\nGPT信任感下降，同时也对其他类别的信息不再信任，破坏社会的知识共\\n享，影响社会的知识交流传播。', metadata={'source': 'chatGPT调研报告.pdf', 'page': 19}), Document(page_content='目前还没有专门针对 ChatGPT 生成文本的正确性进行鉴别的研究论\\n文发表。已有的针对虚假新闻或虚假信息检测的方法可以尝试应用到大规模\\n语言模型生成文本的正确性检测中，比如基于事实抽取和验证的方法。但是\\n基于写作风格的方法可能不太实用，因为大规模语言模型生成文本的过程与\\n人的写作过程有较大区别。\\n隐私泄露风险 隐私泄露风险是指在用户不知情的情况下泄露出自己不想', metadata={'source': 'chatGPT调研报告.pdf', 'page': 19}), Document(page_content='泄露的信息，或者隐私信息被 ChatGPT 通过其他信息推断出来。用户在使\\n用ChatGPT 过程中可能会泄露自己的个人隐私信息或者一些组织乃至国\\n家的机密信息。个人信息的泄露可能会对个人的心理健康、人身安全造成影\\n响。国家或者商业机密往往是只有小范围人员能获悉的高等级信息，它们的\\n20更多⼲货请关注：公众号：历史的光影', metadata={'source': 'chatGPT调研报告.pdf', 'page': 19})]\n","123\n","uploaded\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","from langchain_zhipu import ZhipuAIEmbeddings\n","from langchain_community.chat_models import ChatZhipuAI\n","from dotenv import load_dotenv\n","from langchain_community.vectorstores import Chroma\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain.prompts import PromptTemplate\n","from datasets import Dataset\n","from ragas import evaluate\n","from ragas.metrics import (\n","    faithfulness,\n","    answer_relevancy,\n","    context_recall,\n","    context_precision,\n",")\n","from ragas.metrics.base import Metric\n","\n","# Define character overlap evaluation function\n","def character_overlap_score(generated, ground_truth):\n","    set_generated = set(generated)\n","    set_ground_truth = set(ground_truth)\n","    overlap = set_generated.intersection(set_ground_truth)\n","    return len(overlap) / max(len(set_generated), len(set_ground_truth))\n","\n","class CharacterOverlapMetric(Metric):\n","    @property\n","    def name(self):\n","        return \"character_overlap\"\n","\n","    @staticmethod\n","    def init():\n","        return CharacterOverlapMetric()\n","\n","    def _ascore(self, dataset):\n","        return self.score(dataset)\n","\n","    @property\n","    def evaluation_mode(self):\n","        return \"single\"\n","\n","    def score(self, dataset):\n","        scores = []\n","        for data in dataset:\n","            score = character_overlap_score(data[\"answer\"], data[\"ground_truth\"])\n","            scores.append(score)\n","        return scores\n","\n","# Load environment variables\n","load_dotenv()\n","\n","# Initialize embeddings and other components\n","embeddings = ZhipuAIEmbeddings()\n","chroma = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)\n","retriever = chroma.as_retriever()\n","\n","# Initialize ChatZhipuAI\n","chat = ChatZhipuAI(model=\"glm-4\", temperature=0.5)\n","\n","def format_docs(docs):\n","    return \"\\n\\n\".join(doc.page_content for doc in docs)\n","\n","# Define prompt template\n","prompt = PromptTemplate.from_template(\"Based on {context}, please answer: {question}\")\n","\n","# Define retrieval and generation chain\n","rag_chain = (\n","    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n","    | prompt\n","    | chat\n","    | StrOutputParser()\n",")\n","\n","# Define questions and ground truth answers\n","questions = [\"ChatGPT的优势有哪些?\"]\n","ground_truths = [\"ChatGPT的优势有可玩性,流畅性,自我学习能力,数据标注创新,用户友好,快速响应\"]\n","\n","# Inference\n","answers = []\n","contexts = []\n","\n","# Get answers and context\n","for query in questions:\n","    # Get answers and add to answers list\n","    answers.append(rag_chain.invoke(query))  # Assume it returns a string\n","\n","    # Use invoke to get relevant document content\n","    relevant_documents = retriever.invoke(query)\n","    # Extract relevant document content and add to contexts list\n","    docs_content = [docs.page_content for docs in relevant_documents]\n","    contexts.append(docs_content)  # Add document content list to contexts\n","\n","# Ensure all columns have consistent lengths\n","answers = answers[:len(questions)]\n","contexts = contexts[:len(questions)]\n","\n","data = {\n","    \"question\": questions,\n","    \"answer\": answers,\n","    \"contexts\": contexts,\n","    \"ground_truth\": ground_truths\n","}\n","\n","# Check if all columns have consistent lengths\n","assert len(data['question']) == len(data['answer']) == len(data['contexts']) == len(data['ground_truth']), \"Column lengths are inconsistent\"\n","\n","# Convert dictionary to dataset\n","dataset = Dataset.from_dict(data)\n","\n","# Evaluate with the custom metric included\n","character_overlap_metric = CharacterOverlapMetric()\n","result = evaluate(\n","    dataset,\n","    llm=chat,\n","    embeddings=embeddings,\n","    metrics=[\n","        faithfulness,\n","        context_precision,\n","        context_recall,\n","        answer_relevancy,\n","        character_overlap_metric\n","    ]\n",")\n","\n","# Convert result to pandas DataFrame\n","df = pd.DataFrame(result)\n","print(df)\n","\n","# Plot evaluation results\n","metrics = [\"faithfulness\", \"context_precision\", \"context_recall\", \"answer_relevancy\", \"character_overlap\"]\n","scores = [df[\"faithfulness\"].mean(), df[\"context_precision\"].mean(), df[\"context_recall\"].mean(), df[\"answer_relevancy\"].mean(), df[\"character_overlap\"].mean()]\n","\n","plt.figure(figsize=(10, 6))\n","plt.bar(metrics, scores, color='skyblue')\n","plt.xlabel('Metrics')\n","plt.ylabel('Scores')\n","plt.title('Evaluation Metrics Scores')\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"B1o_QRT73Gf2","executionInfo":{"status":"error","timestamp":1719496635730,"user_tz":-480,"elapsed":16347,"user":{"displayName":"mr li","userId":"02746934671961749824"}},"outputId":"ac717068-41bd-40eb-a8f5-e17547742d59"},"execution_count":70,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"'single'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-70-dc344815eb9e>\u001b[0m in \u001b[0;36m<cell line: 113>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;31m# Evaluate with the custom metric included\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0mcharacter_overlap_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCharacterOverlapMetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m result = evaluate(\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ragas/evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, is_async, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;31m# validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_deprecated_ground_truths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mvalidate_evaluation_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0mvalidate_column_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ragas/validation.py\u001b[0m in \u001b[0;36mvalidate_evaluation_modes\u001b[0;34m(ds, metrics, evalmode_to_columns)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mrequired_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalmode_to_columns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_mode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mavailable_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrequired_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavailable_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'single'"]}]}]}